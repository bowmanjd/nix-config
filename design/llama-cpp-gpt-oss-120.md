# üìö How to replace the `llama-cpp` package from **nixpkgs** with the official `llama.cpp` flake  
*(and get a per‚Äëmachine CUDA / Vulkan / CPU‚Äëoptimised build)*  

Below is a **complete, step‚Äëby‚Äëstep guide** that works with the files you already have.  
It follows modern Nix‚ÄëOS/Flake best‚Äëpractices, keeps everything modular, and removes the old, broken overlay (`./nixos/pkgs/llama-cpp-variants.nix`).

---

## 1. Overview of the required changes

| What we do | Why (and where the information comes from) |
|------------|--------------------------------------------|
| **Add `llama.cpp` as a flake input** | `flake.nix` (File‚ÄØA) already passes `inputs` to all modules via `specialArgs`. Adding a new input is the canonical way to make the external flake visible. |
| **Expose the three variants (CPU, CUDA, Vulkan) as separate packages** | The official flake already builds `default`, `cuda` and `vulkan` (see **File‚ÄØB** ‚Äì `outputs.perSystem.packages`). We only need a thin overlay that re‚Äëexports them under nice names (`llama-cpp‚Äëcpu`, ‚Ä¶) and adds the extra CPU‚Äëoptimisation flags you tried in **File‚ÄØC**. |
| **Add the overlay to the Nixpkgs evaluation used by every host** | `nixpkgs.overlays` is already set in each machine module (File‚ÄØA). We just extend the list with our new overlay. |
| **Select the correct variant per host** | Either (a) define a small boolean in the host‚Äôs `machine/*.nix` file and use a conditional in `environment.systemPackages`, or (b) let a top‚Äëlevel attribute map host‚Äëname ‚Üí package. Both are idiomatic. |
| **Clean up** | Remove the old hand‚Äërolled overlay (`./nixos/pkgs/llama-cpp-variants.nix`). The new overlay completely replaces it. |

---

## 2. Add the official `llama.cpp` flake as an input

Edit **File‚ÄØA** (`flake.nix`) ‚Äì add a new entry in the `inputs` set:

```nix
  inputs = {
    # Nixpkgs
    nixpkgs.url = "github:nixos/nixpkgs/nixos-unstable";
    nixpkgs-stable.url = "github:nixos/nixpkgs/nixos-25.05";

    # ‚Ä¶ (other inputs you already have) ‚Ä¶

    # ‚Üê NEW: llama.cpp
    llama-cpp = {
      url = "github:ggml-org/llama.cpp";
      # make it follow the same nixpkgs you already use, otherwise you get a
      # second copy of nixpkgs for the flake‚Äôs own overlays.
      inputs.nixpkgs.follows = "nixpkgs";
    };
  };
```

*Why*: The flake‚Äôs `outputs` contain an overlay (`llama-cpp.overlays.default`) and ready‚Äëmade packages (`default`, `cuda`, `vulkan`). By pulling the flake as an input we can reuse both.

---

## 3. Create a **tiny, dedicated overlay** that re‚Äëexports the three variants and adds the CPU‚Äëoptimisation flags you want

Create a new file, e.g. `overlays/llama-cpp.nix` (feel free to place it wherever you keep other overlays).

```nix
# overlays/llama-cpp.nix
#
# 1Ô∏è‚É£ Re‚Äëexport the three variants that the upstream flake already builds:
#    * default ‚Üí pure‚ÄëCPU (no GPU)
#    * cuda    ‚Üí CUDA‚Äëaccelerated
#    * vulkan  ‚Üí Vulkan‚Äëaccelerated
#
# 2Ô∏è‚É£ Provide a *CPU‚Äëoptimised* variant that turns on the instruction‚Äëset
#    flags you listed in your previous overlay (File‚ÄØC).

{ inputs, system }:

let
  # Pull the per‚Äësystem package set from the llama.cpp flake
  llamaPkgs = inputs.llama-cpp.packages.${system};

  # Base CPU package (no GPU).  We keep it as `llama-cpp-cpu-base` for
  # readability; the final name we expose will be `llama-cpp-cpu`.
  llamaCpuBase = llamaPkgs.default;

  # Apply the extra CMake flags that you tried in File‚ÄØC.
  # We use `.overrideAttrs` only because the upstream derivation does **not**
  # expose those flags as top‚Äëlevel arguments.
  llamaCpuOptimised = llamaCpuBase.overrideAttrs (old: {
    cmakeFlags = old.cmakeFlags ++ [
      "-DGGML_NATIVE=ON"
      "-DGGML_SSE42=ON"
      "-DGGML_AVX=ON"
      "-DGGML_F16C=ON"
      "-DGGML_AVX2=ON"
      "-DGGML_BMI2=ON"
      "-DGGML_FMA=ON"
      "-DGGML_AVX512=ON"
      "-DGGML_AVX512_VBMI=ON"
      "-DGGML_AVX512_VNNI=ON"
      "-DGGML_OPENMP=ON"
      "-DLLAMA_BUILD_SERVER=ON"
    ];
  });

in
{
  # The overlay entry ‚Äì it will be added to `nixpkgs.overlays`.
  # `final` is the pkgs set that already contains the overlay‚Äëchain,
  # `prev` is the set *without* our overlay.
  overlays = final: prev: {
    # Keep the original llama‚Äëcpp name for people who expect it.
    llama-cpp = llamaPkgs.default;      # plain CPU (no extra flags)

    # Public names we will use in the system / home‚Äëmanager modules:
    llama-cpp-cpu    = llamaCpuOptimised;   # your tuned CPU build
    llama-cpp-cuda   = llamaPkgs.cuda;      # CUDA variant
    llama-cpp-vulkan = llamaPkgs.vulkan;    # Vulkan variant
  };
}
```

### What this does

* **No duplicate builds** ‚Äì we reuse the packages the upstream flake already builds (`default`, `cuda`, `vulkan`).  
* **CPU optimisation** ‚Äì we only add the extra `cmakeFlags` once, via `overrideAttrs`.  
* **Clear names** ‚Äì `llama-cpp-cpu`, `llama-cpp-cuda`, `llama-cpp-vulkan`. You can still refer to `llama-cpp` if you need the vanilla CPU version.

---

## 4. Wire the new overlay into every NixOS configuration

You have two ways:

* **A. Add it directly in each host‚Äôs module list** ‚Äì this is the smallest change.
* **B. Put it into a shared module** (e.g. `./nixos/overlays.nix`) ‚Äì keeps the host files cleaner.

Both approaches are shown; pick the one you like.

### 4.A. Directly in each host (quick start)

Edit each host entry in **File‚ÄØA** (`flake.nix`). In the `modules = [ ‚Ä¶ ]` list, after the existing block that adds `overlay-stable`, add the new overlay:

```nix
          ({
            config,
            pkgs,
            ...
          }: {
            nixpkgs.overlays = [
              overlay-stable
              # ‚Üê NEW: llama‚Äëcpp overlay (we pass `inputs` and `system` explicitly)
              (import ./overlays/llama-cpp.nix {
                inherit inputs;
                system = "x86_64-linux";   # or aarch64-linux for the ARM hosts
              })
            ];
          })
```

Do the same for every host (`carbon`, `beryllium`, `boron`, `nitrogen`).  

*Why the `system` argument?*  
The overlay we wrote expects the system architecture to pick the correct `llama-cpp` sub‚Äëpackage (`${system}`).

### 4.B. Centralised overlay module (recommended for many hosts)

Create a tiny module `./nixos/llama-cpp-overlay.nix`:

```nix
# ./nixos/llama-cpp-overlay.nix
{ inputs, ... }:

{
  nixpkgs.overlays = [
    (import ./../overlays/llama-cpp.nix {
      inherit inputs;
      system = builtins.currentSystem;   # works because the module is evaluated in the same system
    })
  ];
}
```

Then, in **File‚ÄØA**, replace the per‚Äëhost overlay block with a reference to the shared module:

```nix
          inputs.home-manager.nixosModules.home-manager
          ./nixos/llama-cpp-overlay.nix   # ‚Üê NEW
          inputs.nixos-hardware.nixosModules.framework-13-7040-amd
```

Now every host automatically gets the overlay, and you only have to maintain a single line.

---

## 5. Selecting the right variant per machine

### 5.1. Decide on a simple ‚ÄúGPU‚Äëtype‚Äù attribute

Add a small option to each machine‚Äôs config file (`machines/*.nix`). For example, in `machines/carbon.nix` (the machine that has an NVIDIA GPU) you could add:

```nix
{ lib, ... }:
{
  # a custom attribute inside `hardware` (you could pick any namespace)
  hardware.gpu = lib.mkDefault "cuda";   # possible values: "cuda", "vulkan", "cpu"
}
```

For a Vulkan‚Äëonly laptop (`beryllium.nix`):

```nix
{ lib, ... }:
{
  hardware.gpu = lib.mkDefault "vulkan";
}
```

And for a pure‚ÄëCPU workstation (`boron.nix`):

```nix
{ lib, ... }:
{
  hardware.gpu = lib.mkDefault "cpu";
}
```

### 5.2. Use that flag in the system package list

Add the following to **every** host (or put it into a shared module if you prefer) ‚Äì it picks the right package from `pkgs` based on the attribute we just set:

```nix
{
  config,
  lib,
  pkgs,
  ...
}:

let
  gpuKind = config.hardware.gpu or "cpu";   # fallback safe
  llamaPkg = builtins.attrNames {
    cpu    = pkgs.llama-cpp-cpu;
    cuda   = pkgs.llama-cpp-cuda;
    vulkan = pkgs.llama-cpp-vulkan;
  }.${gpuKind};
in
{
  environment.systemPackages = with pkgs; [
    llamaPkg
    # other packages ‚Ä¶
  ];
}
```

**Explanation**  
* `gpuKind` reads the custom attribute you set in the host file.  
* `builtins.attrNames` is a tiny trick to map `"cpu"` ‚Üí `pkgs.llama-cpp-cpu`, etc., without an `if` chain.  

If you use **Home Manager** instead of system packages, the same expression works inside `home.packages`:

```nix
home.packages = with pkgs; [
  llamaPkg
];
```

### 5.3. Optional: central mapping in the flake outputs

If you prefer to keep host‚Äëspecific logic *outside* the NixOS modules, you can expose a map from hostname ‚Üí package in `flake.nix`:

```nix
outputs = { self, inputs, ... }@inputs: let
  # ‚Ä¶ (your existing let‚Äëbindings)
  mkLlamaPkg = system: gpuKind:
    let pkgs = import inputs.nixpkgs {
          inherit system;
          overlays = [ overlay-stable (import ./overlays/llama-cpp.nix { inherit inputs system; }) ];
        };
    in
    pkgs."llama-cpp-${gpuKind}";
in {
  # expose a convenience attribute:
  hostLlamaPkg = {
    carbon    = mkLlamaPkg "x86_64-linux" "cuda";
    beryllium = mkLlamaPkg "x86_64-linux" "vulkan";
    boron     = mkLlamaPkg "x86_64-linux" "cpu";
    nitrogen  = mkLlamaPkg "x86_64-linux" "cpu";
  };
  # ‚Ä¶ rest of your outputs
}
```

Then inside a host module you can simply write:

```nix
environment.systemPackages = [ inputs.self.hostLlamaPkg.${config.networking.hostName} ];
```

Both approaches are idiomatic; pick whichever keeps your repo clearer.

---

## 6. Home‚ÄØManager usage (if you prefer user‚Äëlevel packages)

Add the same overlay to the Home‚ÄØManager `pkgs` set by enabling `useGlobalPkgs = true` (which you already do) and then reference the package in `home.packages`:

```nix
{
  # inside your home-manager module
  home.packages = with pkgs; [
    # The same conditional trick can be used here, or you can hard‚Äëcode
    # the variant you always want for a given user.
    llama-cpp-cpu
  ];
}
```

If you want the per‚Äëhost variant, you can read the host attribute via `builtins.getEnv "HOSTNAME"` (set by the system) or expose it via `specialArgs` ‚Äì but the system‚Äëlevel approach from ¬ß5 already guarantees the correct binary is installed system‚Äëwide, which most users prefer.

---

## 7. Clean‚Äëup: remove the old, broken overlay

Delete the file you created earlier:

```bash
rm ./nixos/pkgs/llama-cpp-variants.nix
```

Also remove any references to `inputs.llama-cpp` or `llama-cpp.packages` that might still be lingering in your personal overlays ‚Äì they are no longer needed.

---

## 8. Full, minimal **final `flake.nix`** (showing the relevant parts only)

Below is a **condensed** version of your original `flake.nix` with the new input, the overlay wiring, and the per‚Äëhost GPU flag handling. You can copy‚Äëpaste it and only adjust the parts that differ for your own repo (e.g., other inputs).

```nix
{
  description = "techyporcupine's NixOS Config!";

  inputs = {
    # ‚îÄ‚îÄ Existing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    nixpkgs.url = "github:nixos/nixpkgs/nixos-unstable";
    nixpkgs-stable.url = "github:nixos/nixpkgs/nixos-25.05";
    nixos-hardware.url = "github:NixOS/nixos-hardware/master";

    waybar = {
      url = "github:Alexays/Waybar/master";
      inputs.nixpkgs.follows = "nixpkgs";
    };
    home-manager = {
      url = "